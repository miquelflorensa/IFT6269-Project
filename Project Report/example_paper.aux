\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lecun2015deep}
\citation{kendall2017uncertainties}
\citation{blundell15}
\citation{}
\citation{neal12}
\citation{gal16}
\citation{lakshminarayanan17}
\citation{blundell15}
\citation{goulet2020tractable}
\citation{}
\newlabel{sec:intro}{{1}{1}{}{section.1}{}}
\citation{gal16}
\citation{well11}
\citation{neal12}
\citation{blundell15}
\citation{NIPS2017}
\citation{NIPS2011}
\newlabel{sec:related_work}{{2}{2}{}{section.2}{}}
\citation{hobbhahn22a}
\citation{goulet2020tractable}
\citation{goulet2020tractable}
\citation{goulet2020tractable}
\newlabel{sec:methodology}{{3}{3}{}{section.3}{}}
\newlabel{sub:tagi}{{3.1}{3}{}{subsection.3.1}{}}
\newlabel{eq:TAGI}{{3.1}{3}{}{subsection.3.1}{}}
\newlabel{fig:FNN}{{1}{3}{(a) A neural network representation. (b) A compacted NN representation, reproduced from \cite {goulet2020tractable}}{figure.1}{}}
\citation{goulet2020tractable}
\citation{goulet2020tractable}
\citation{goulet2020tractable}
\citation{goulet2020tractable}
\newlabel{fig:updateTAGI}{{2}{4}{Recursive layer-wise inference, reproduced from \cite {goulet2020tractable}. Cyan arrows indicate inference directions}{figure.2}{}}
\newlabel{eq:z0update}{{3.1}{4}{}{figure.2}{}}
\newlabel{eq:layerwiseupdate}{{3.1}{4}{}{figure.2}{}}
\newlabel{sub:classification_tagi}{{4.1}{4}{}{subsection.4.1}{}}
\newlabel{eq_cov_z_e_check}{{8}{5}{}{equation.4.8}{}}
\newlabel{eq_log_act}{{9}{5}{}{equation.4.9}{}}
\newlabel{eq_act}{{11}{5}{}{equation.4.11}{}}
\newlabel{fig:remax_4}{{3}{6}{Ratio of predictions and epistemic uncertainty associated to each class on a MNIST \textit {4} image}{figure.3}{}}
\newlabel{fig:remax_frog}{{4}{6}{Ratio of predictions and epistemic uncertainty associated to each class on a CIFAR10 \textit {frog} image}{figure.4}{}}
\newlabel{sec:tagi_vs_torch}{{6}{6}{}{section.6}{}}
\newlabel{tab:overall_performance}{{1}{7}{Overall Average Training and Test Error Rates (\%) across Frameworks}{table.1}{}}
\newlabel{tab:batch_size_impact}{{2}{7}{Test Error Rates (\%) by Batch Size and Framework (FNN and CNN only)}{table.2}{}}
\newlabel{tab:batch_norm_batch_size_impact}{{3}{7}{Test Error Rates (\%) by Batch Size, Batch Normalization, and Framework (CNN only)}{table.3}{}}
\newlabel{fig:num_layers_comparison}{{5}{8}{Test Error Rate by Number of Layers for TAGI-HRC vs. Torch. TAGI-HRC maintains low error rates as depth increases, while Torch performance degrades significantly with deeper networks (especially CNNs without batch normalization)}{figure.5}{}}
\newlabel{fig:tagi_sigma}{{6}{8}{Impact of $\sigma _v$ on TAGI Performance across Different Network Depths. Higher $\sigma _v$ values consistently improve test error rates, especially in deeper networks}{figure.6}{}}
\newlabel{tab:fnn_neurons_impact}{{4}{8}{FNN Test Error (\%) by Neurons per Layer (Averaged across Layers)}{table.4}{}}
\newlabel{tab:cnn_channels_impact}{{5}{8}{CNN Test Error (\%) by Initial Channels per Layer (Averaged)}{table.5}{}}
\citation{langley00}
\bibdata{example_paper}
\bibcite{blundell15}{{1}{2015}{{Blundell et~al.}}{{Blundell, Cornebise, Kavukcuoglu, and Wierstra}}}
\bibcite{DudaHart2nd}{{2}{2000}{{Duda et~al.}}{{Duda, Hart, and Stork}}}
\bibcite{gal16}{{3}{2016}{{Gal \& Ghahramani}}{{Gal and Ghahramani}}}
\bibcite{goulet2020tractable}{{4}{2021}{{Goulet et~al.}}{{Goulet, Nguyen, and Amiri}}}
\bibcite{NIPS2011}{{5}{2011}{{Graves}}{{}}}
\bibcite{hobbhahn22a}{{6}{2022}{{Hobbhahn et~al.}}{{Hobbhahn, Kristiadi, and Hennig}}}
\bibcite{kearns89}{{7}{1989}{{Kearns}}{{}}}
\bibcite{NIPS2017}{{8}{2017}{{Lakshminarayanan et~al.}}{{Lakshminarayanan, Pritzel, and Blundell}}}
\bibcite{langley00}{{9}{2000}{{Langley}}{{}}}
\newlabel{tab:training_time}{{6}{9}{Training Time (minutes) vs. Test Error (\%) by Number of Layers}{table.6}{}}
\bibcite{MachineLearningI}{{10}{1983}{{Michalski et~al.}}{{Michalski, Carbonell, and Mitchell}}}
\bibcite{mitchell80}{{11}{1980}{{Mitchell}}{{}}}
\bibcite{neal12}{{12}{2012}{{Neal}}{{}}}
\bibcite{Newell81}{{13}{1981}{{Newell \& Rosenbloom}}{{Newell and Rosenbloom}}}
\bibcite{Samuel59}{{14}{1959}{{Samuel}}{{}}}
\bibcite{well11}{{15}{2011}{{Welling \& Teh}}{{Welling and Teh}}}
\bibstyle{icml2018}
\gdef \@abspage@last{10}
